\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[margin=2.54cm]{geometry}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{xr}
\usepackage[section]{placeins}
%\usepackage{hyperref}

\externaldocument{hw2_code}

\definecolor{matlabgreen}{RGB}{28,172,0}
\definecolor{matlablilas}{RGB}{170,55,241}

\newcommand{\includecode}[1]{\lstinputlisting[caption={\ttfamily #1.m},label={lst:#1}]{matlab/#1.m}}
\newcommand{\inlinecode}[1]{\lstinline[basicstyle=\ttfamily,keywordstyle={},stringstyle={},commentstyle={\itshape}]{#1}}

\renewcommand{\vec}[1]{\underline{#1}}
\renewcommand{\Re}[1]{\operatorname{Re}\left[#1\right]}
\newcommand{\E}[1]{\operatorname{E}\left[#1\right]}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}

\author{Enrico Polo \and Riccardo Zanol}
\title{Homework 2}

\begin{document}
\lstset{
  language=Matlab,
  basicstyle={\ttfamily \footnotesize},
  breaklines=true,
  morekeywords={true,false,warning,xlim,ylim},
  keywordstyle=\color{blue},
  stringstyle=\color{matlablilas},
  commentstyle={\color{matlabgreen} \itshape},
  numberstyle={\ttfamily \tiny},
  frame=leftline,
  showstringspaces=false,
  numbers=left,
  upquote=true,
}
\maketitle
\section*{Problem 1}
\section*{Problem 2}
To prove equation (3.271) of the textbook we start from the definition
of $J(k) = \E{\abs{e(k)}^2}$ and substitute
\[ e(k) = d(k) - y(k) = \left(\vec{h}^T - \vec{c}^T(k) \right) \vec{x}(k) + w(k) \]
inside the expectation. So the mean squared error becomes
\begin{equation}
  J(k) = \E{\abs{\left(\vec{h}^T - \vec{c}^T(k) \right) \vec{x}(k)}^2}
  + \E{\abs{w(k)}^2}
  + \E{2\Re{\left(\vec{h}^T - \vec{c}^T(k) \right) \vec{x}(k) w^*(k)}}
  \label{eq:J}
\end{equation}
{\color{red} where the last term is zero because ...}.

The second term of equation (\ref{eq:J}) is just $\sigma^2_w$, while
the first expectation can be written as
\[ \E{
  \left(\sum_{i=0}^{N_{max}}\left(h_i - c_i\right)x(k-i)\right)
  \left(\sum_{j=0}^{N_{max}}\left(h_j - c_j\right)x(k-j)\right)^* } \]
where $N_{max} = \max\{N, N_h\}$ and we assume that all the vectors
are padded with zeros to length $N_{max}$.  If we rearrange the sums
and the expectation then, {\color{red} if we assume that the pairs
  $(c_i,c_j)$ and $(x(k-i),x(k-j))$ are statistically independent
  $\forall i,j$}, we can factor the expectation:
\begin{equation}
%%   &\sum_{i=0}^{N_{max}}\sum_{j=0}^{N_{max}}
%% \E{(h_i -c_i)(h_j-c_j)^*x(k-i)x^*(k-j)} \\
\sum_{i=0}^{N_{max}}\sum_{j=0}^{N_{max}}
\E{(h_i -c_i)(h_j-c_j)^*}
\E{x(k-i)x^*(k-j)} .
\label{eq:J_sums}
\end{equation}
Now if the input $x(k)$ is white its autocorrelation is
\[
\E{x(k-i)x^*(k-j)} = r_x(j-i) = r_x(0)\delta(j-i)
\]
so the only non-zero terms of equation (\ref{eq:J_sums}) are those
with $i=j$ and it becomes:
\begin{equation}
  r_x(0)\E{\norm{\vec{h}-\vec{c}(k)}^2}
  \label{eq:J_final_diff}
\end{equation}

Depending on the order of the channel and the filter there are two cases:
\begin{enumerate}
\item If $N \geq N_h$ the optimal value of $\vec{c}(k)$ is
  $\vec{c}_{opt} = \vec{h}$, so the difference in equation
  (\ref{eq:J_final_diff}) can be written as $\vec{h}-\vec{c}(k) =
  -\left(\vec{c}(k)-\vec{c}_{opt}\right) = -\vec{\Delta c}(k)$ and the
  MSE is:
  \begin{equation}
    J(k) = \sigma^2_w + r_x(0)\E{\norm{\vec{\Delta c}(k)}^2}
    = J_{min} + r_x(0)\E{\norm{\vec{\Delta c}(k)}^2}.
  \end{equation}
  \item If $N < N_h$ the filter can only model the first $N$
    coefficients of the channel and the optimal solution is
    $\vec{c}_{opt} = [h_0,h_1,\dots h_{N-1},0,\dots 0]^T$. If we write
    the difference of equation (\ref{eq:J_final_diff}) using the
    residual error vector $ \vec{\Delta h}(\infty) =
    [0,\dots0,-h_{N},-h_{N+1},\dots-h_{N_h-1}]^T $ we get
    $\vec{h}-\vec{c}(k) = -\vec{\Delta c}(k) -\vec{\Delta h}(\infty)$.
    Computing the expectation of equation (\ref{eq:J_final_diff}) by
    writing the norm term by term and bringing out the sum we get
    \[
    \sum_{i=0}^{N_h}\E{
      \abs{\Delta h_i(\infty)}^2
      + \abs{\Delta c_i(k)}^2
      + 2\Re{\Delta h_i(\infty) \Delta c_i^*(k)}
    }
    \]
    where we notice that, in the last term, one of either $\Delta
    h_i(\infty)$ or $\Delta c_i(k)$ must be zero, so what is left is
    $\norm{\vec{\Delta h}(\infty)}^2 + \E{\norm{\vec{\Delta c}(k)}^2}$
    and, putting everything together, we get the MSE as
    \begin{equation}
      J(k) = \sigma^2_w + r_x(0)\norm{\vec{\Delta h}(\infty)}^2 + r_x(0)\E{\norm{\vec{\Delta c}(k)}^2}
      = J_{min} + r_x(0)\E{\norm{\vec{\Delta c}(k)}^2} .
    \end{equation}
\end{enumerate}

To prove equation (3.272) we start from equation (3.70) and assume
that:
\begin{itemize}
\item $\vec{\Delta c}(k)$ and $\vec{x}(k)$ are independent, 
\item $e_{min}(k)$ and $\vec{x}(k)$ are orthogonal, 
\item $\vec{x}^T(k)\vec{x}^*(k) \approx Nr_x(0)$.
\end{itemize}
In the LMS algorithm the error on the coefficient vector changes at each iteration in this way (3.70):
\begin{equation*}
  \vec{\Delta c}(k+1) = \left( I - \mu\vec{x}^*(k)\vec{x}^T(k) \right) \vec{\Delta c}(k) + \mu e_{min}(k)\vec{x}^*(k) , 
\end{equation*}
and, if we take the expectation of the squared norm of both terms, we get
\begin{align}
  \E{\norm{\vec{\Delta c}(k+1)}^2} % &=  \E{\norm{\left( I - \mu\vec{x}^*(k)\vec{x}^T(k) \right) \vec{\Delta c}(k) + \mu e_{min}(k)\vec{x}^*(k)}^2} \\
  &= \E{\norm{\left( I - \mu\vec{x}^*(k)\vec{x}^T(k) \right) \vec{\Delta c}(k)}^2}
  \label{eq:exp_1} \\
  & \quad + \E{\norm{\mu e_{min}(k)\vec{x}^*(k)}^2}
  \label{eq:exp_2} \\
  & \quad + \E{\left(\left( I - \mu\vec{x}^*(k)\vec{x}^T(k) \right)\vec{\Delta c}(k)\right)^T\left(\mu e_{min}(k)\vec{x}^*(k)\right)^*}
  \label{eq:exp_3} \\
  & \quad + \E{\left(\mu e_{min}(k)\vec{x}^*(k)\right)^T\left(\left( I - \mu\vec{x}^*(k) \vec{x}^T(k)\right) \vec{\Delta c}(k)\right)^*}
  \label{eq:exp_4} .
\end{align}

If we write the norm term by term the first expectation
(\ref{eq:exp_1}) becomes
\[ \E{\sum_{i=0}^{N-1}\abs{1 - \mu\abs{x(k-i)}^2}^2\abs{\Delta c_i(k)}^2} \]
then we compute the product between the two squared absolute values,
we bring the expectaion inside the finite sum and we exploit the
independece between $x(k-i)$ and $\Delta c_i(k)$ to factor the
expectations so the first term becomes:
\[ \E{\norm{\vec{\Delta c}(k)}^2} +
\mu^2\sum_{i=0}^{N-1}\E{\abs{x(k-i)}^4}\E{\abs{\Delta c_i(k)}^2} - 2\mu
r_x(0)\E{\norm{\vec{\Delta c}(k)}^2}
\]
where we also exploited the WSS of $x(k)$ to write $\E{\abs{x(k-i)}^2}
= r_x(0)$. Now the fourth moment of $x(k)$ would be $3\sigma^4_x$, but
we approximate it with $r_x(0)^2$ like in equation (3.78) of the
textbook because we would otherwise get slightly different results.
So in the end we have the following expression:
\begin{equation}
\E{\norm{\vec{\Delta c}(k)}^2} +
\mu^2r_x(0)^2\E{\norm{\vec{\Delta c}(k)}^2} - 2\mu
r_x(0)\E{\norm{\vec{\Delta c}(k)}^2} .
\label{eq:c_final_1}
\end{equation}

If we use the approximation $\norm{\vec{x}(k)}^2 \approx Nr_x(0)$ in
the second expectation (\ref{eq:exp_2}), it becomes
\begin{equation}
\mu^2 \E{\abs{e_{min}(k)}^2 \norm{\vec{x}^*(k)}^2}
= \mu^2Nr_x(0)\E{\abs{e_{min}(k)}^2}
= \mu^2Nr_x(0)J_{min} .
\label{eq:c_final_2}
\end{equation}

The last two expectations (\ref{eq:exp_3}) and (\ref{eq:exp_4}) can be
rewritten as, respectively,
\begin{align*}
  & \E{\left(\mu - \mu^2\norm{\vec{x}(k)}^2\right) e_{min}^*(k) \vec{\Delta c}^T(k)\vec{x}(k)} \\
  & \E{\left(\mu - \mu^2\norm{\vec{x}(k)}^2\right) e_{min}(k) \vec{\Delta c}^T(k)\vec{x}(k)}
\end{align*}
    {\color{red} anche qua non saprei piÃ¹ come fare}

Putting together equations (\ref{eq:c_final_1}), (\ref{eq:c_final_2})
and (\ref{eq:c_final_3}) we get the difference equation
\begin{equation}
  \E{\norm{\vec{\Delta c}(k+1)}^2} = \gamma \E{\norm{\vec{\Delta
        c}(k)}^2} + \mu^2Nr_x(0)J_{min}
\end{equation}
where we define $\gamma$ from equation (\ref{eq:c_final_1}) by
collecting the constant coefficients of $\E{\norm{\vec{\Delta c}(k)}^2}$:
\[ \gamma = 1 + \mu^2r_x(0)^2 - 2\mu r_x(0) . \]
The solution to this difference equation for $k \geq 0$ is
\begin{equation}
  \E{\norm{\vec{\Delta c}(k)}^2} =
  \gamma^k \E{\norm{\vec{\Delta c}(0)}^2} +
  \mu^2Nr_x(0)J_{min} \frac{1 - \gamma^k}{1 - \gamma}
\end{equation}
which is what we wanted to prove.
\section*{Problem 3}
\end{document}
