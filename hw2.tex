\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[margin=2.54cm]{geometry}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{xr}
\usepackage[section]{placeins}
%\usepackage{hyperref}

\externaldocument{hw2_code}

\definecolor{matlabgreen}{RGB}{28,172,0}
\definecolor{matlablilas}{RGB}{170,55,241}

\newcommand{\includecode}[1]{\lstinputlisting[caption={\ttfamily #1.m},label={lst:#1}]{matlab/#1.m}}
\newcommand{\inlinecode}[1]{\lstinline[basicstyle=\ttfamily,keywordstyle={},stringstyle={},commentstyle={\itshape}]{#1}}

\renewcommand{\vec}[1]{\underline{#1}}
\renewcommand{\Re}[1]{\operatorname{Re}\left[#1\right]}
\newcommand{\E}[1]{\operatorname{E}\left[#1\right]}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}

\author{Enrico Polo \and Riccardo Zanol}
\title{Homework 2}

\begin{document}
\lstset{
  language=Matlab,
  basicstyle={\ttfamily \footnotesize},
  breaklines=true,
  morekeywords={true,false,warning,xlim,ylim},
  keywordstyle=\color{blue},
  stringstyle=\color{matlablilas},
  commentstyle={\color{matlabgreen} \itshape},
  numberstyle={\ttfamily \tiny},
  frame=leftline,
  showstringspaces=false,
  numbers=left,
  upquote=true,
}
\maketitle
\section*{Problem 1}
To estimate the filter impulse response $\vec{h}^T = [h(0), ..., h(N-1)]$ by using a FIR filter of order $N_{fil}$ we use a PN sequence $\vec{p}^T = [p(0),...p(L-1)]$ (taking values in the set \{-1;1\}) periodic of period $L\geq N$ with $N=15$ that is the upper bound of the order of $h$. To take into account the transient of the filter in analysis we partially repeat the input sequence adding N-1 samples, putting as input a sequence $\vec{x}^T = [p(0), p(1),...,p(L-1),p(0),...,p(N-2))]$. So we get that the output of the unknown system (including the noise) is given by 
\begin{align*}
d(k)& = \vec{h}^T \vec{x} + w(k)&  k& = N-1,N-2...,N-1+L-1
\end{align*}
????????At the receiver side we consider $d(k)$ as the desired process we want to trace using our FIR filter, using $\vec{p}$ as input. In this way we can find that setting properly $N_{fil}$ and $L$ $\vec{c}_{opt} = \vec{h}$ in the ideal case (same realisation of the white input at the sender and at the receiver) and approximates it using a PN sequence. ??????????
\newline Applying the correlation method we can find an estimate of the filter coefficients by using the relation 
\begin{align*}
\hat{r_{dx}}(n) &= \frac{1}{L} \sum_{k=N-1}^{(N-1) + (L-1)} {d(k)p^*(k-n)} & n&=0,1,...N_{fil} -1
\end{align*}
in which $d(k)$ is a received sequence of length L and $p(k)$ is the input PN sequence of length L, known by the receiver. ????????If we try an $N_{fil} \geq N$ the index of $p(k-n)$ must be negative in the computation of the sum but we can already compute that by using the property that for periodicity $p(-i) = p(L-i)$?????????????. The error commited in this case is given by ???????$d-\hat{d} or R^{-1}..$??????

Using instead the LS method we compute the autocorrelation matrix of the PN sequence $\phi$ whose elements are 
\begin{align*}
\phi(i,n) &= \sum_{k=N-1}^{(N-1) + (L-1)}{p(k-i)p^*(k-n)}&
i,n&=0,1,...N_{fil} -1
\end{align*}
and the cross correlation vector between $\vec{d}$ and $\vec{p}$, $\vec{\theta}^T = [\theta(0), \theta(1),..., \theta(N_{fil}-1)]$ whose elements are
\begin{align*}
\theta(n)& = \sum_{k=N-1}^{(N-1) + (L-1)}{d(k)p^*(k-n)}& n=0,1...,N_{fil}-1
\end{align*}
Then we find te optimal solution for the LS problem that give us  $\vec{\hat{h_{ls}}} = \vec{\phi}^{-1} \vec{\theta}$ with the correspondent error 
$ e_{min} = e_d - \vec{\theta}^H \vec{\hat{h}}_{ls}$ ($e_d$ energy of the desired signal $d(k)$). Using this, we can finally estimate the power of the noise that affects $d(k)$ by $\hat{\sigma_\omega^2} = \frac{e_{min}}{L}$.
\newline In our tries we tried to find the optimal values $N_{fil}$ and $L$. We started from $L = 15$ increasing it until the error $e$ didn't increase any longer. With both methods we find that from L=63 or L=127, it depends on the specific realization of w(k), and $N_{fil}$ fixed, we get the same results in therms of $\hat{\sigma_\omega^2} = e/L\simeq -5 dB$. Regarding the estimation of the impulse response we have to increase the value of $L$ at least to 255, to have quite good results as can be seen in Fig. ~\ref{plot:h}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/h_r8}
  \caption{PSDs comparison (Impulse response: theoretical and estimated)}
    \label{plot:h}
\end{figure}
So finally we decided to stop here in order to find a trade-off between the computation cost and good results. Furthermore we observe that, as espected, the variance on the estimation of $h$ and $\hat{\sigma_\omega^2}$ decreases while $L$ becomes larger and also for this reason we decided for $L=255$ samples.

$\Lambda_n = \frac{\sigma_\omega^2}{M_x || h-\hat{h} ||^2}11111$
$\Lambda_n^{corr} = \frac{L}{N_{fil} + 1/L \bigl[ \Lambda + (N_{fil}-2)\bigl(\frac{|H(0)|^2}{\sigma_\omega^2} \bigr)\bigr]}$
$\Lambda_n^{ls} = \frac{(L+1)(L+1-N_{fil})}{N_{fil}(L+2-N_{fil})}$
$\Lambda = \frac{M_x||\vec{h}||^2}{\sigma_\omega^2}111$


\section*{Problem 2}
To prove equation (3.271) of the textbook we start from the definition
of $J(k) = \E{\abs{e(k)}^2}$ and substitute
\[ e(k) = d(k) - y(k) = \left(\vec{h}^T - \vec{c}^T(k) \right) \vec{x}(k) + w(k) \]
inside the expectation. So the mean squared error becomes
\begin{equation}
  J(k) = \E{\abs{\left(\vec{h}^T - \vec{c}^T(k) \right) \vec{x}(k)}^2}
  + \E{\abs{w(k)}^2}
  + \E{2\Re{\left(\vec{h}^T - \vec{c}^T(k) \right) \vec{x}(k) w^*(k)}}
  \label{eq:J}
\end{equation}
{\color{red} where the last term is zero because ...}.

The second term of equation (\ref{eq:J}) is just $\sigma^2_w$, while
the first expectation can be written as
\[ \E{
  \left(\sum_{i=0}^{N_{max}}\left(h_i - c_i\right)x(k-i)\right)
  \left(\sum_{j=0}^{N_{max}}\left(h_j - c_j\right)x(k-j)\right)^* } \]
where $N_{max} = \max\{N, N_h\}$ and we assume that all the vectors
are padded with zeros to length $N_{max}$.  If we rearrange the sums
and the expectation then, {\color{red} if we assume that the pairs
  $(c_i,c_j)$ and $(x(k-i),x(k-j))$ are statistically independent
  $\forall i,j$}, we can factor the expectation:
\begin{equation}
%%   &\sum_{i=0}^{N_{max}}\sum_{j=0}^{N_{max}}
%% \E{(h_i -c_i)(h_j-c_j)^*x(k-i)x^*(k-j)} \\
\sum_{i=0}^{N_{max}}\sum_{j=0}^{N_{max}}
\E{(h_i -c_i)(h_j-c_j)^*}
\E{x(k-i)x^*(k-j)} .
\label{eq:J_sums}
\end{equation}
Now if the input $x(k)$ is white its autocorrelation is
\[
\E{x(k-i)x^*(k-j)} = r_x(j-i) = r_x(0)\delta(j-i)
\]
so the only non-zero terms of equation (\ref{eq:J_sums}) are those
with $i=j$ and it becomes:
\begin{equation}
  r_x(0)\E{\norm{\vec{h}-\vec{c}(k)}^2}
  \label{eq:J_final_diff}
\end{equation}

Depending on the order of the channel and the filter there are two cases:
\begin{enumerate}
\item If $N \geq N_h$ the optimal value of $\vec{c}(k)$ is
  $\vec{c}_{opt} = \vec{h}$, so the difference in equation
  (\ref{eq:J_final_diff}) can be written as $\vec{h}-\vec{c}(k) =
  -\left(\vec{c}(k)-\vec{c}_{opt}\right) = -\vec{\Delta c}(k)$ and the
  MSE is:
  \begin{equation}
    J(k) = \sigma^2_w + r_x(0)\E{\norm{\vec{\Delta c}(k)}^2}
    = J_{min} + r_x(0)\E{\norm{\vec{\Delta c}(k)}^2}.
  \end{equation}
  \item If $N < N_h$ the filter can only model the first $N$
    coefficients of the channel and the optimal solution is
    $\vec{c}_{opt} = [h_0,h_1,\dots h_{N-1},0,\dots 0]^T$. If we write
    the difference of equation (\ref{eq:J_final_diff}) using the
    residual error vector $ \vec{\Delta h}(\infty) =
    [0,\dots0,-h_{N},-h_{N+1},\dots-h_{N_h-1}]^T $ we get
    $\vec{h}-\vec{c}(k) = -\vec{\Delta c}(k) -\vec{\Delta h}(\infty)$.
    Computing the expectation of equation (\ref{eq:J_final_diff}) by
    writing the norm term by term and bringing out the sum we get
    \[
    \sum_{i=0}^{N_h}\E{
      \abs{\Delta h_i(\infty)}^2
      + \abs{\Delta c_i(k)}^2
      + 2\Re{\Delta h_i(\infty) \Delta c_i^*(k)}
    }
    \]
    where we notice that, in the last term, one of either $\Delta
    h_i(\infty)$ or $\Delta c_i(k)$ must be zero, so what is left is
    $\norm{\vec{\Delta h}(\infty)}^2 + \E{\norm{\vec{\Delta c}(k)}^2}$
    and, putting everything together, we get the MSE as
    \begin{equation}
      J(k) = \sigma^2_w + r_x(0)\norm{\vec{\Delta h}(\infty)}^2 + r_x(0)\E{\norm{\vec{\Delta c}(k)}^2}
      = J_{min} + r_x(0)\E{\norm{\vec{\Delta c}(k)}^2} .
    \end{equation}
\end{enumerate}

To prove equation (3.272) we start from equation (3.70) and assume
that:
\begin{itemize}
\item $\vec{\Delta c}(k)$ and $\vec{x}(k)$ are independent, 
\item $e_{min}(k)$ and $\vec{x}(k)$ are orthogonal, 
\item $\vec{x}^T(k)\vec{x}^*(k) \approx Nr_x(0)$.
\end{itemize}
In the LMS algorithm the error on the coefficient vector changes at each iteration in this way (3.70):
\begin{equation*}
  \vec{\Delta c}(k+1) = \left( I - \mu\vec{x}^*(k)\vec{x}^T(k) \right) \vec{\Delta c}(k) + \mu e_{min}(k)\vec{x}^*(k) , 
\end{equation*}
and, if we take the expectation of the squared norm of both terms, we get
\begin{align}
  \E{\norm{\vec{\Delta c}(k+1)}^2} % &=  \E{\norm{\left( I - \mu\vec{x}^*(k)\vec{x}^T(k) \right) \vec{\Delta c}(k) + \mu e_{min}(k)\vec{x}^*(k)}^2} \\
  &= \E{\norm{\left( I - \mu\vec{x}^*(k)\vec{x}^T(k) \right) \vec{\Delta c}(k)}^2}
  \label{eq:exp_1} \\
  & \quad + \E{\norm{\mu e_{min}(k)\vec{x}^*(k)}^2}
  \label{eq:exp_2} \\
  & \quad + \E{\left(\left( I - \mu\vec{x}^*(k)\vec{x}^T(k) \right)\vec{\Delta c}(k)\right)^T\left(\mu e_{min}(k)\vec{x}^*(k)\right)^*}
  \label{eq:exp_3} \\
  & \quad + \E{\left(\mu e_{min}(k)\vec{x}^*(k)\right)^T\left(\left( I - \mu\vec{x}^*(k) \vec{x}^T(k)\right) \vec{\Delta c}(k)\right)^*}
  \label{eq:exp_4} .
\end{align}

If we write the norm term by term the first expectation
(\ref{eq:exp_1}) becomes
\[ \E{\sum_{i=0}^{N-1}\abs{1 - \mu\abs{x(k-i)}^2}^2\abs{\Delta c_i(k)}^2} \]
then we compute the product between the two squared absolute values,
we bring the expectaion inside the finite sum and we exploit the
independece between $x(k-i)$ and $\Delta c_i(k)$ to factor the
expectations so the first term becomes:
\[ \E{\norm{\vec{\Delta c}(k)}^2} +
\mu^2\sum_{i=0}^{N-1}\E{\abs{x(k-i)}^4}\E{\abs{\Delta c_i(k)}^2} - 2\mu
r_x(0)\E{\norm{\vec{\Delta c}(k)}^2}
\]
where we also exploited the WSS of $x(k)$ to write $\E{\abs{x(k-i)}^2}
= r_x(0)$. {\color{red} e adesso? supponiamo l'input bianco? supponiamo qualche forma per il 4o momento?}

If we use the approximation $\norm{\vec{x}(k)}^2 \approx Nr_x(0)$ in
the second expectation (\ref{eq:exp_2}), it becomes
\[
\mu^2 \E{\abs{e_{min}(k)}^2 \norm{\vec{x}^*(k)}^2}
= \mu^2Nr_x(0)\E{\abs{e_{min}(k)}^2}
= \mu^2Nr_x(0)J_{min} .
\]

The last two expectations (\ref{eq:exp_3}) and (\ref{eq:exp_4}) can be
rewritten as, respectively,
\begin{align*}
  & \E{\left(\mu - \mu^2\norm{\vec{x}(k)}^2\right) e_{min}^*(k) \vec{\Delta c}^T(k)\vec{x}(k)} \\
  & \E{\left(\mu - \mu^2\norm{\vec{x}(k)}^2\right) e_{min}(k) \vec{\Delta c}^T(k)\vec{x}(k)}
\end{align*}
{\color{red} anche qua non saprei piÃ¹ come fare}
\section*{Problem 3}
\end{document}
